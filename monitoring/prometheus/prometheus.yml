global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - 'rules/*.yml'

scrape_configs:
  - job_name: 'dcgm-exporter'
    static_configs:
      - targets: ['dcgm-exporter:9400']
    metric_relabel_configs:
      - source_labels: [gpu]
        regex: (.*)
        target_label: gpu_index
        replacement: '${1}'

  - job_name: 'ml-workload'
    static_configs:
      - targets: ['ml-workload:8000']
    metric_relabel_configs:
      - source_labels: [model_name]
        regex: (.*)
        target_label: model
        replacement: '${1}'

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # alertmanager:9093

# Recording rules for ML metrics
groups:
  - name: ml_workload_rules
    rules:
      - record: ml:inference_time:avg_5m
        expr: rate(ml_inference_time_seconds_sum[5m]) / rate(ml_inference_time_seconds_count[5m])
      
      - record: ml:token_rate:avg_5m
        expr: rate(ml_tokens_processed_total[5m])
      
      - record: gpu:memory_utilization:avg_5m
        expr: avg_over_time(DCGM_FI_DEV_FB_USED[5m]) / DCGM_FI_DEV_FB_TOTAL * 100
      
      - record: gpu:power_efficiency
        expr: ml_tokens_processed_total / (DCGM_FI_DEV_POWER_USAGE * 3600)  # tokens per watt-hour

  - name: alert_rules
    rules:
      - alert: HighGPUUtilization
        expr: DCGM_FI_DEV_GPU_UTIL > 95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPU Utilization > 95%"
          description: "GPU {{ $labels.gpu }} has been at high utilization for 5 minutes"

      - alert: HighGPUTemperature
        expr: DCGM_FI_DEV_GPU_TEMP > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPU Temperature > 80°C"
          description: "GPU {{ $labels.gpu }} temperature is {{ $value }}°C"

      - alert: LowInferenceThroughput
        expr: rate(ml_inference_time_seconds_count[5m]) < 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low inference throughput"
          description: "Inference rate has dropped below 1 per second"